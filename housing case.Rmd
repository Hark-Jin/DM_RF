---
title: "Housing case"
author: "He Jin"
date: "11/6/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rsample)      # data splitting 
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)
library(AmesHousing)
```

```{r}
set.seed(123)
ames_split = initial_split(AmesHousing::make_ames(), prop = .7)
ames_train = training(ames_split)
ames_test  = testing(ames_split)
```

```{r}
set.seed(123)

# default RF model
m1 <- randomForest(
  formula = Sale_Price ~ .,
  data    = ames_train
)
plot(m1)
which.min(m1$mse)
sqrt(m1$mse[which.min(m1$mse)])
```

```{r}
# create training and validation data 
set.seed(123)
valid_split = initial_split(ames_train, .8)

# training data
ames_train_v2 = analysis(valid_split)

# validation data
ames_valid = assessment(valid_split)
x_test = ames_valid[setdiff(names(ames_valid), "Sale_Price")]
y_test = ames_valid$Sale_Price

rf_oob_comp = randomForest(
  formula = Sale_Price ~ .,
  data    = ames_train_v2,
  xtest   = x_test,
  ytest   = y_test)

# extract OOB & validation errors
oob = sqrt(rf_oob_comp$mse)
validation = sqrt(rf_oob_comp$test$mse)

# compare error rates
tibble::tibble(
  `Out of Bag Error` = oob,
  `Test error` = validation,
  ntrees = 1:rf_oob_comp$ntree
) %>%
  gather(Metric, RMSE, -ntrees) %>%
  ggplot(aes(ntrees, RMSE, color = Metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  xlab("Number of trees")
```

```{r}
# names of features
features = setdiff(names(ames_train), "Sale_Price")

set.seed(123)

m2 = tuneRF(
  x          = ames_train[features],
  y          = ames_train$Sale_Price,
  ntreeTry   = 500,   # number of trees used at the tuning step
  mtryStart  = 5,     # starting value of mtry
  stepFactor = 1.5,   # inflation setting for next mtry
  improve    = 0.01,  # the minimum improvment required 
  trace      = T      # to show real-time progress
  )
```

```{r}
ames_ranger <- ranger(
    formula   = Sale_Price ~ ., 
    data      = ames_train, 
    num.trees = 500,
    mtry      = floor(length(features) / 3))
```

```{r}
# hyperparameter grid search
hyper_grid <- expand.grid(
  mtry       = seq(20, 30, by = 2),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE   = 0
)

# total number of combinations
nrow(hyper_grid)
```

```{r}
for(i in 1:nrow(hyper_grid)) {
  # train model
  model <- ranger(
    formula         = Sale_Price ~ ., 
    data            = ames_train, 
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = 123
  )
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}
hyper_grid[which.min(hyper_grid$OOB_RMSE),]
```

```{r}
OOB_RMSE <- vector(mode = "numeric", length = 100)

for(i in seq_along(OOB_RMSE)) {

  optimal_ranger <- ranger(
    formula         = Sale_Price ~ ., 
    data            = ames_train, 
    num.trees       = 500,
    mtry            = 20,
    min.node.size   = 5,
    sample.fraction = .8,
    importance      = 'impurity'
  )
  
  OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)
```

```{r}
optimal_ranger$variable.importance %>% 
  tidy() %>%
  dplyr::arrange(desc(x)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(reorder(names, x), x)) +
  geom_col() +
  coord_flip() +
  ggtitle("Top 25 important variables")
```

```{r}
h2o.no_progress()
h2o.init(max_mem_size = "5g")

# create feature names
y <- "Sale_Price"
x <- setdiff(names(ames_train), y)

# turn training set into h2o object
train.h2o <- as.h2o(ames_train)

# hyperparameter grid
hyper_grid.h2o <- list(
  ntrees      = seq(200, 500, by = 150),
  mtries      = seq(15, 35, by = 10),
  max_depth   = seq(20, 40, by = 5),
  min_rows    = seq(1, 5, by = 2),
  nbins       = seq(10, 30, by = 5),
  sample_rate = c(.55, .632, .75)
)

# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 5*60
  )

# build grid search 
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid",
  x = x, 
  y = y, 
  training_frame = train.h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = search_criteria
  )

# collect the results and sort by our model performance metric of choice
grid_perf2 <- h2o.getGrid(
  grid_id = "rf_grid", 
  sort_by = "mse", 
  decreasing = FALSE
  )
print(grid_perf2)

# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)

# Now letâ€™s evaluate the model performance on a test set
ames_test.h2o <- as.h2o(ames_test)
best_model_perf <- h2o.performance(model = best_model, newdata = ames_test.h2o)

# RMSE of best model
rmse_h2o = h2o.mse(best_model_perf) %>% sqrt()
```

```{r}
# RMSE 
rmse <- function(error)
{
  sqrt(mean(error^2))
}

ames_randomForest <- randomForest(
    formula = Sale_Price ~ ., 
    data    = ames_train, 
    ntree   = 500,
    mtry    = 22)

pred_randomForest <- predict(ames_randomForest, ames_test)
error.rf = ames_test$Sale_Price - pred_randomForest
rmse_rf = rmse(error.rf)

pred_ranger <- predict(optimal_ranger, ames_test)
error.ranger = ames_test$Sale_Price - pred_ranger$predictions
rmse_ranger = rmse(error.ranger)

h2o.mse(best_model_perf) %>% sqrt()

test_mse = c(rmse_rf, rmse_ranger, rmse_h2o)
model_name = c("Random Forest", "Ranger", "H2o")
table_test_error_all = cbind(model_name, test_mse)
library(pander)
as.tibble(table_test_error_all)
pander(table_test_error_all)
kabble
```

